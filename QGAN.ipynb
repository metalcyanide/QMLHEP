{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1yRdQUL63eS"
   },
   "source": [
    "## TASK 2: Quantum Generative Adversarial Network (QGAN) \n",
    "\n",
    "Dataset: QIS_EXAM_200Events.npz\n",
    "\n",
    "- This notebook consists of working implementation of QGAN for the given dataset and at the end tensorboard profiling is shown\n",
    "- Used code snippets from tutorials of tensorflow Quantum\n",
    "- Skip the following block, if tensorflow and tensorflow quantum are installed already.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w0XWlvmRFuX8",
    "outputId": "3be9c1c8-050c-4af4-e608-6887d2d88619"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.3.1\n",
    "!pip install tensorflow_quantum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pGdHrm4OF0RQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_quantum as tfq\n",
    "from tensorflow.keras import layers\n",
    "import cirq\n",
    "import sympy\n",
    "import numpy as np\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "# visualization tools\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from cirq.contrib.svg import SVGCircuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HO8VPHDT7UdZ"
   },
   "source": [
    "### Loading Data\n",
    "\n",
    "Get training data, labels and test data and corresponding labels\n",
    "\n",
    "The .npz file consists of zero-dimensional array, accessing which requires expanding its dimensions along axis=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8L1247y-F1Rn",
    "outputId": "88db54e0-5b27-4fd9-d7d5-9be885ff85b6"
   },
   "outputs": [],
   "source": [
    "data = np.load(\"/content/QIS_EXAM_200Events.npz\", allow_pickle=True)\n",
    "\n",
    "for k in data:\n",
    "    print(k)\n",
    "total_data = dict(zip((\"train_data\",\"test_data\"), (data[k] for k in data)))\n",
    "total_data = np.expand_dims(total_data,axis=0)\n",
    "\n",
    "train_data = total_data[0][\"train_data\"]\n",
    "test_data = total_data[0][\"test_data\"]\n",
    "training_data_signal = np.expand_dims(train_data,axis=0)[0]['1']\n",
    "training_data_background = np.expand_dims(train_data,axis=0)[0]['0']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJavoWjzG5Rb"
   },
   "outputs": [],
   "source": [
    "def generate_circuit(qubits):\n",
    "    \"\"\"Generate a random circuit on qubits.\"\"\"\n",
    "    random_circuit = cirq.generate_boixo_2018_supremacy_circuits_v2(\n",
    "        qubits, cz_depth=2, seed=1234)\n",
    "    return random_circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPV1Zn3bHicU"
   },
   "outputs": [],
   "source": [
    "N_QUBITS = 5\n",
    "QUBITS = cirq.GridQubit.rect(1, N_QUBITS)\n",
    "REFERENCE_CIRCUIT = generate_circuit(QUBITS)\n",
    "all_data = training_data_signal + training_data_background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y06zW1-_7cjy"
   },
   "source": [
    "Creating Generator and discriminator models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YjQzO387Hqco"
   },
   "outputs": [],
   "source": [
    "LATENT_DIM = 100\n",
    "def make_generator_model():\n",
    "    \"\"\"Construct generator model.\"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(256, use_bias=False, input_shape=(LATENT_DIM,)))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(N_QUBITS, activation='relu'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def make_discriminator_model():\n",
    "    \"\"\"Constrcut discriminator model.\"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(256, use_bias=False, input_shape=(N_QUBITS,)))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CP72jqIPHvzi"
   },
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "generator = make_generator_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBuQffweH3L1"
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    \"\"\"Compute discriminator loss.\"\"\"\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    \"\"\"Compute generator loss.\"\"\"\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9fZverOH5Te"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=256\n",
    "\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    \"\"\"Run train step on provided image batch.\"\"\"\n",
    "    noise = tf.random.normal([BATCH_SIZE, LATENT_DIM])\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        # print(images.shape)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(\n",
    "        gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(\n",
    "        disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(\n",
    "        zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(\n",
    "        zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7uLZfOYIFIp"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "logdir = \"tb_logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(logdir + \"/metrics\")\n",
    "file_writer.set_as_default()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvitCIrQ7ldQ"
   },
   "source": [
    "The Generator loss and Discriminator loss can be vizualized using tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6OjOEuzIGxx"
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs, start_epoch=1):\n",
    "    \"\"\"Launch full training run for the given number of epochs.\"\"\"\n",
    "    # Log original training distribution.\n",
    "    # tf.summary.histogram('Training Distribution', data=bits_to_ints(dataset), step=0)\n",
    "\n",
    "    batched_data = tf.data.Dataset.from_tensor_slices(dataset).shuffle(100).batch(512)\n",
    "    t = time.time()\n",
    "    for epoch in range(start_epoch, start_epoch + epochs):\n",
    "        for i, image_batch in enumerate(batched_data):\n",
    "            # Log batch-wise loss.\n",
    "            gl, dl = train_step(image_batch)\n",
    "            tf.summary.scalar(\n",
    "                'Generator loss', data=gl, step=epoch * len(batched_data) + i)\n",
    "            tf.summary.scalar(\n",
    "                'Discriminator loss', data=dl, step=epoch * len(batched_data) + i)\n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch {}, took {}(s)'.format(epoch, time.time() - t))\n",
    "            t = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 821
    },
    "id": "8_uj3O_GIL0s",
    "outputId": "cb10935f-1482-48ee-a1b3-09877327326e"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir tb_logs/\n",
    "\n",
    "logdir = \"tb_logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(logdir + \"/metrics\")\n",
    "file_writer.set_as_default()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9XgFyxw7vCa"
   },
   "source": [
    "Profiling can also be visualized using tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_HBeBtLpIOLT",
    "outputId": "824857ca-080f-48e8-c550-5315f136d8ae"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "tf.profiler.experimental.start(logdir)\n",
    "train(all_data, epochs=100, start_epoch=10)\n",
    "tf.profiler.experimental.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OpVKBhTkIPnH"
   },
   "outputs": [],
   "source": [
    "test_data_signal = np.expand_dims(test_data,axis=0)[0]['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4ddoYM4vxdb"
   },
   "outputs": [],
   "source": [
    "val_signal = discriminator(test_data_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZeF0JfzKv0KI",
    "outputId": "11319adf-ec1c-4011-a907-a0c17290c46a"
   },
   "outputs": [],
   "source": [
    "print(f\"accuracy: \" + str(sum([1 if i>0 else 0 for i in val_signal])/len(val_signal)))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "QGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
